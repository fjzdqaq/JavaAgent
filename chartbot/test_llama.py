#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•vLLM Meta Llama 3.3 70Bæ¨¡å‹
"""

import requests
import json
import time

def test_vllm_model():
    """
    æµ‹è¯•vLLMæ¨¡å‹API
    """
    # æ¨¡å‹é…ç½®
    config = {
        "endpoint": "http://10.2.4.153:80/v1",
        "model": "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4"
    }
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•vLLM Meta Llama 3.3 70Bæ¨¡å‹")
    print("=" * 60)
    print(f"ğŸ“¡ ç«¯ç‚¹: {config['endpoint']}")
    print(f"ğŸ¤– æ¨¡å‹: {config['model']}")
    print("=" * 60)
    
    # æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        {
            "role": "user",
            "content": "ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        }
    ]
    
    # æ„é€ è¯·æ±‚æ•°æ®
    request_data = {
        "model": config["model"],
        "messages": test_messages,
        "max_tokens": 512,
        "temperature": 0.7,
        "stream": False
    }
    
    try:
        print("ğŸ“¤ å‘é€è¯·æ±‚...")
        print(f"ğŸ’¬ ç”¨æˆ·æ¶ˆæ¯: {test_messages[0]['content']}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # å‘é€POSTè¯·æ±‚
        response = requests.post(
            f"{config['endpoint']}/chat/completions",
            headers={
                "Content-Type": "application/json",
            },
            json=request_data,
            timeout=60
        )
        
        # è®°å½•å“åº”æ—¶é—´
        response_time = time.time() - start_time
        
        print(f"â±ï¸  å“åº”æ—¶é—´: {response_time:.2f}ç§’")
        print(f"ğŸ“Š çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            
            # æå–æ¨¡å‹å›å¤
            if "choices" in result and len(result["choices"]) > 0:
                assistant_message = result["choices"][0]["message"]["content"]
                
                print("âœ… æ¨¡å‹å“åº”æˆåŠŸ!")
                print("-" * 40)
                print("ğŸ¤– Llama 3.3å›å¤:")
                print(f"ğŸ“ {assistant_message}")
                print("-" * 40)
                
                # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
                if "usage" in result:
                    usage = result["usage"]
                    print("ğŸ“ˆ ä½¿ç”¨ç»Ÿè®¡:")
                    print(f"   è¾“å…¥tokens: {usage.get('prompt_tokens', 'N/A')}")
                    print(f"   è¾“å‡ºtokens: {usage.get('completion_tokens', 'N/A')}")
                    print(f"   æ€»è®¡tokens: {usage.get('total_tokens', 'N/A')}")
                
                return True
            else:
                print("âŒ å“åº”æ ¼å¼é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°choices")
                return False
                
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            return False
            
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶")
        print("ğŸ’¡ æç¤º: æ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´åŠ è½½æˆ–æ¨ç†")
        return False
        
    except requests.exceptions.ConnectionError:
        print("âŒ è¿æ¥é”™è¯¯")
        print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç«¯ç‚¹åœ°å€æ˜¯å¦æ­£ç¡®")
        return False
        
    except Exception as e:
        print(f"âŒ å‡ºç°æœªçŸ¥é”™è¯¯: {e}")
        return False


def test_model_info():
    """
    è·å–æ¨¡å‹ä¿¡æ¯
    """
    endpoint = "http://10.2.4.153:80/v1"
    
    try:
        print("\nğŸ” è·å–æ¨¡å‹ä¿¡æ¯...")
        response = requests.get(f"{endpoint}/models", timeout=10)
        
        if response.status_code == 200:
            models = response.json()
            print("ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
            if "data" in models:
                for model in models["data"]:
                    print(f"   â€¢ {model.get('id', 'Unknown')}")
            else:
                print("   (æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨)")
        else:
            print(f"âŒ æ— æ³•è·å–æ¨¡å‹ä¿¡æ¯: {response.status_code}")
            
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯æ—¶å‡ºé”™: {e}")


def test_advanced_conversation():
    """
    æµ‹è¯•æ›´å¤æ‚çš„å¯¹è¯
    """
    config = {
        "endpoint": "http://10.2.4.153:80/v1",
        "model": "ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4"
    }
    
    print("\nğŸ§  æµ‹è¯•é«˜çº§å¯¹è¯èƒ½åŠ›...")
    
    # æ›´å¤æ‚çš„æµ‹è¯•é—®é¢˜
    test_questions = [
        "è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°ï¼Œè¦æ±‚ä½¿ç”¨é€’å½’å®ç°ã€‚",
        "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆï¼Œä»¥åŠå¦‚ä½•é˜²æ­¢è¿‡æ‹Ÿåˆï¼Ÿ",
        "å¦‚æœä½ æ˜¯ä¸€ä¸ªé¡¹ç›®ç»ç†ï¼Œå¦‚ä½•ç®¡ç†ä¸€ä¸ª5äººçš„å¼€å‘å›¢é˜Ÿï¼Ÿ"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ æµ‹è¯•é—®é¢˜ {i}: {question}")
        
        request_data = {
            "model": config["model"],
            "messages": [{"role": "user", "content": question}],
            "max_tokens": 300,
            "temperature": 0.7
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{config['endpoint']}/chat/completions",
                headers={"Content-Type": "application/json"},
                json=request_data,
                timeout=30
            )
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                if "choices" in result and len(result["choices"]) > 0:
                    answer = result["choices"][0]["message"]["content"]
                    print(f"âœ… å›ç­” ({response_time:.2f}s):")
                    print(f"ğŸ¤– {answer[:200]}{'...' if len(answer) > 200 else ''}")
                else:
                    print("âŒ å“åº”æ ¼å¼é”™è¯¯")
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    print("ğŸ”¬ vLLM Meta Llama 3.3 70B æ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    # åŸºç¡€è¿æ¥æµ‹è¯•
    success = test_vllm_model()
    
    if success:
        # è·å–æ¨¡å‹ä¿¡æ¯
        test_model_info()
        
        # é«˜çº§å¯¹è¯æµ‹è¯•
        test_advanced_conversation()
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    else:
        print("\nâŒ åŸºç¡€æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡åç»­æµ‹è¯•")
    
    print("=" * 60) 